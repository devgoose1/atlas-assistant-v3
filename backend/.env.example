# JARVIS Assistant Configuration
# Copy this file to .env and adjust values as needed

# API Settings
APP_NAME=JARVIS Assistant
APP_VERSION=1.0.0
HOST=0.0.0.0
PORT=8000
DEBUG=false
CORS_ORIGINS=*

# LLM Provider Settings
# Options: "ollama" or "llamacpp"
LLM_PROVIDER=ollama

# For Ollama (development)
# LLM_MODEL_NAME=qwen:1.5b-chat-v1.5-q4_0
# LLM_BASE_URL=http://localhost:11434

# For llama.cpp (production on Raspberry Pi)
# LLM_PROVIDER=llamacpp
# LLM_MODEL_NAME=qwen-1_5b-chat-q4_0.gguf
# LLM_BASE_URL=http://localhost:8080

# LLM Generation Settings
LLM_MAX_TOKENS=256
LLM_TEMPERATURE=0.1

# Performance Settings
REQUEST_TIMEOUT=30
MAX_CONTEXT_LENGTH=1500

# Feature Flags
ENABLE_COMMAND_ROUTING=true
ENABLE_INTENT_CLASSIFICATION=true
